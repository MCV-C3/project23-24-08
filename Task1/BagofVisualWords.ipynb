{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Luis/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import os, tqdm\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skimage.feature import fisher_vector, learn_gmm\n",
    "\n",
    "from typing import *\n",
    "\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_ENTITY\"] = \"c3-mcv\"\n",
    "wandb.login(key = '14a56ed86de5bf43e377d95d05458ca8f15f5017', relogin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first read the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils functions\n",
    "def read_pickle(filepath:str) -> Any:\n",
    "    \"\"\"\n",
    "    Read and deserialize a pickled object from the specified file.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The path to the file containing the pickled object.\n",
    "\n",
    "    Returns:\n",
    "        Any: The deserialized object from the pickled file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        load_file = pickle.load(file)\n",
    "\n",
    "    return load_file\n",
    "\n",
    "def write_pickle(information:Any,filepath:str):\n",
    "    \"\"\"\n",
    "    Serialize and write an object to the specified file using pickle.\n",
    "\n",
    "    Parameters:\n",
    "        information (Any): The object to be serialized and written.\n",
    "        filepath (str): The path to the file to write the pickled object.\n",
    "    \"\"\"\n",
    "\n",
    "    abs_path = os.path.dirname(filepath)\n",
    "    os.makedirs(abs_path, exist_ok=True)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(information, f)\n",
    "\n",
    "def split_image_into_blocks(image, level):\n",
    "    \n",
    "    # Initialize the list to store blocks\n",
    "    blocks = []\n",
    "    # Process each level\n",
    "    for l in range(1, level + 1):\n",
    "        # Calculate the number of splits along each axis\n",
    "        splits = 2 ** (l - 1)\n",
    "\n",
    "        # Split the image into blocks using np.array_split along both axes\n",
    "        row_blocks = np.array_split(image, splits, axis=0)\n",
    "        for row_block in row_blocks:\n",
    "            col_blocks = np.array_split(row_block, splits, axis=1)\n",
    "            for col_block in col_blocks:\n",
    "                blocks.append(col_block)\n",
    "\n",
    "    return blocks \n",
    "\n",
    "def histogram_intersection_kernel(X, Y):\n",
    "    \"\"\"\n",
    "    Histogram intersection kernel.\n",
    "    \n",
    "    Args:\n",
    "    X: array-like of shape (n_samples_X, n_features)\n",
    "    Y: array-like of shape (n_samples_Y, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    kernel_matrix: array of shape (n_samples_X, n_samples_Y)\n",
    "    \"\"\"\n",
    "    # Expand dimensions of X and Y for broadcasting\n",
    "    X_expanded = np.expand_dims(X, 1)\n",
    "    Y_expanded = np.expand_dims(Y, 0)\n",
    "\n",
    "    # Compute the minimum between each pair of vectors (broadcasting)\n",
    "    minima = np.minimum(X_expanded, Y_expanded)\n",
    "\n",
    "    # Sum over the feature dimension to compute the kernel\n",
    "    kernel_matrix = np.sum(minima, axis=2)\n",
    "\n",
    "    return kernel_matrix\n",
    "\n",
    "def histogram_intersection_distance(X, Y):\n",
    "    \"\"\"\n",
    "    Histogram intersection distance for kNN.\n",
    "    \n",
    "    Args:\n",
    "    X: array-like of shape (n_samples_X, n_features)\n",
    "    Y: array-like of shape (n_samples_Y, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    distance_matrix: array of shape (n_samples_X, n_samples_Y)\n",
    "    \"\"\"\n",
    "    # Calculate the histogram intersection similarity\n",
    "    similarity = histogram_intersection_kernel(X, Y)\n",
    "    \n",
    "    max_similarity = np.minimum(X.sum(axis=1)[:, np.newaxis], Y.sum(axis=1)[np.newaxis, :])\n",
    "    return 1 - (similarity / max_similarity)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return sum(predictions == labels) / len(labels)\n",
    "\n",
    "def precision(predictions, labels, class_label):\n",
    "    tp = np.sum((predictions == class_label) & (labels == class_label))\n",
    "    fp = np.sum((predictions == class_label) & (labels != class_label))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def recall(predictions, labels, class_label):\n",
    "    tp = np.sum((predictions == class_label) & (labels == class_label))\n",
    "    fn = np.sum((predictions != class_label) & (labels == class_label))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def average_precision(predictions, labels):\n",
    "    classes = np.unique(labels)\n",
    "    return np.mean([precision(predictions, labels, c) for c in classes])\n",
    "\n",
    "def average_recall(predictions, labels):\n",
    "    classes = np.unique(labels)\n",
    "    return np.mean([recall(predictions, labels, c) for c in classes])\n",
    "\n",
    "def average_f1(predictions, labels):\n",
    "    return 2 * average_precision(predictions, labels) * average_recall(predictions, labels) / (average_precision(predictions, labels) + average_recall(predictions, labels))\n",
    "\n",
    "def compute_macro_roc_curve(y_onehot_test, y_score):\n",
    "    \"\"\"\n",
    "    Computes the ROC curve and ROC area for each class.\n",
    "    \n",
    "    Args:\n",
    "    y_onehot_test: array-like of shape (n_samples, n_classes)\n",
    "    prob_matrix: array-like of shape (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "    fpr: dict of false positive rates for each class\n",
    "    tpr: dict of true positive rates for each class\n",
    "    roc_auc: dict of ROC AUC values for each class\n",
    "    \"\"\"\n",
    "    n_classes = y_onehot_test.shape[1]\n",
    "    # store the fpr, tpr\n",
    "    fpr, tpr = dict(), dict()\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    return fpr_grid, mean_tpr / n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BoVW():\n",
    "    def __init__(self, config, size_per_class=1e9, preload_features=False, folder_path_train='./MIT_split/train', folder_path_test='./MIT_split/test'):\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        self.train_dataset = {'image_paths': [], 'labels': []}\n",
    "        self.test_dataset = {'image_paths': [], 'labels': []}\n",
    "\n",
    "        for label in os.listdir(folder_path_train):\n",
    "            for i,image_name in enumerate(os.listdir(os.path.join(folder_path_train, label))):\n",
    "                self.train_dataset['image_paths'].append(os.path.join(folder_path_train, label, image_name))\n",
    "                self.train_dataset['labels'].append(label)\n",
    "\n",
    "                # good for making a small test\n",
    "                if i >= size_per_class:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        for label in os.listdir(folder_path_test):\n",
    "            for i,image_name in enumerate(os.listdir(os.path.join(folder_path_test, label))):\n",
    "                self.test_dataset['image_paths'].append(os.path.join(folder_path_test, label, image_name))\n",
    "                self.test_dataset['labels'].append(label)\n",
    "                \n",
    "                # good for making a small test\n",
    "                if i >= size_per_class:\n",
    "                    break\n",
    "        \n",
    "        self.train_dataset['labels'] = np.array(self.train_dataset['labels'])\n",
    "        self.test_dataset['labels'] = np.array(self.test_dataset['labels'])\n",
    "        self.train_features_cluster, self.train_features_level, self.test_features_level = self._compute_features(preload=preload_features)\n",
    "        \n",
    "        # Classification\n",
    "        if self.config['classifier'] == 'knn':\n",
    "            self.classifier = KNeighborsClassifier(n_neighbors=self.config['n_neigh'], n_jobs=-1, metric=self.config['metric'])\n",
    "        elif self.config['classifier'] == 'svm':\n",
    "            self.classifier = SVC(kernel = self.config['kernel'], degree=self.config['degree_pol'], gamma = 'auto', C = self.config['C'], probability=True, random_state=123)\n",
    "        elif self.config['classifier'] == 'logistic':\n",
    "            self.classifier = LogisticRegression(multi_class = 'auto', penalty='l2', solver='lbfgs', C = self.config['C'], n_jobs=-1, random_state=123)\n",
    "\n",
    "        self.dim_red = None\n",
    "        if self.config['n_components'] > 0:\n",
    "            self.dim_red = PCA(n_components = self.config['n_components'])\n",
    "\n",
    "        # Standarization\n",
    "        self.scaler = None\n",
    "        if self.config['scaler']:\n",
    "            self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "\n",
    "    def _compute_descriptor_level(self, gray_chunk, color_chunk):\n",
    "\n",
    "        if self.config['descriptor'] == 'dense_sift':\n",
    "            # Initialize Dense SIFT extractor\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp = [cv2.KeyPoint(x, y, self.config['step_size']) for y in range(0, gray_chunk.shape[0], self.config['step_size'])\n",
    "                                                                for x in range(0, gray_chunk.shape[1], self.config['step_size'])]\n",
    "            _, des = sift.compute(gray_chunk, kp)\n",
    "\n",
    "        elif self.config['descriptor'] == 'sift':\n",
    "            # Initialize Dense SIFT extractor\n",
    "            sift = cv2.SIFT_create(nfeatures=self.config[\"n_features\"])\n",
    "            _, des = sift.detectAndCompute(gray_chunk, None)\n",
    "\n",
    "        elif self.config['descriptor'] == 'akaze':\n",
    "            # Initialize AKAZE extractor\n",
    "            akaze = cv2.AKAZE_create(descriptor_type=cv2.AKAZE_DESCRIPTOR_KAZE, threshold=0.0001)\n",
    "            _, des = akaze.detectAndCompute(color_chunk, None)\n",
    "          \n",
    "        return des\n",
    "\n",
    "    def _compute_features(self, preload=False):\n",
    "        \n",
    "        if preload:\n",
    "            level_features_train = read_pickle(filepath=os.path.join(\".\", \"descriptors\", \"train\", self.config[\"descriptor\"]+'_'+ str(self.config[\"level_pyramid\"]) + \".pkl\"))\n",
    "            level_features_test = read_pickle(filepath=os.path.join(\".\", \"descriptors\", \"test\", self.config[\"descriptor\"]+'_'+ str(self.config[\"level_pyramid\"]) + \".pkl\"))\n",
    "            cluster_features_train = np.concatenate([np.concatenate(image_feat_array, axis=0) for image_feat_array in level_features_train], axis=0)\n",
    "            return cluster_features_train, level_features_train, level_features_test\n",
    "\n",
    "        cluster_features_train = []\n",
    "        level_features_train, level_features_test = [], []\n",
    "        # Iterate over all files in the directory\n",
    "        for i,dataset in enumerate([self.train_dataset['image_paths'], self.test_dataset['image_paths']]):\n",
    "            for filename in tqdm.tqdm(dataset, desc='Extracting features from dataset %d' % i):\n",
    "                \n",
    "                # Load the imag\n",
    "                img = cv2.imread(filename)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img_blocks = split_image_into_blocks(img, self.config['level_pyramid'])\n",
    "                gray_blocks = split_image_into_blocks(gray, self.config['level_pyramid'])\n",
    "\n",
    "                level_features = [] # levels x np.array([n_features, 128])\n",
    "                for c_block, g_block in zip(img_blocks, gray_blocks):\n",
    "                    des = self._compute_descriptor_level(g_block, c_block)\n",
    "\n",
    "                    if des is not None:\n",
    "                        level_features.append(des.astype(np.float64)) # may contain different number of features for each level\n",
    "                    else:\n",
    "                        hist, _ = np.histogramdd(c_block.reshape(-1, 3), bins = 128 if self.config['descriptor'] != 'akaze' else 64)\n",
    "                        level_features.append((hist.sum(axis=1).sum(axis=0) / hist.sum()).reshape(1,-1).astype(np.float64))\n",
    "                \n",
    "                cluster_features = np.concatenate(level_features, axis=0) # shape (levels * n_features, 128), n_features does not need to be the same for each level\n",
    "\n",
    "                if i == 0:\n",
    "                    cluster_features_train.append(cluster_features)\n",
    "                    level_features_train.append(level_features)\n",
    "                else:\n",
    "                    level_features_test.append(level_features)\n",
    "\n",
    "        # saving features -> from level_features we can get the cluster_features\n",
    "        os.makedirs(os.path.join(\".\", \"descriptors\", \"train\"), exist_ok=True)\n",
    "        write_pickle(filepath=os.path.join(\".\", \"descriptors\", \"train\", self.config[\"descriptor\"]+'_'+ str(self.config[\"level_pyramid\"]) + \".pkl\"), information=level_features_train)\n",
    "        \n",
    "        os.makedirs(os.path.join(\".\", \"descriptors\", \"test\"), exist_ok=True)\n",
    "        write_pickle(filepath=os.path.join(\".\", \"descriptors\", \"test\", self.config[\"descriptor\"]+'_'+ str(self.config[\"level_pyramid\"]) + \".pkl\"), information=level_features_test)\n",
    "        return np.concatenate(cluster_features_train, axis=0), level_features_train, level_features_test\n",
    "\n",
    "    def fit(self, train_features_cluster=None, train_features_level=None, y_train_labels=None):\n",
    "\n",
    "        if train_features_cluster is None and train_features_level is None and y_train_labels is None:\n",
    "            train_features_cluster = self.train_features_cluster\n",
    "            train_features_level = self.train_features_level\n",
    "            y_train_labels = self.train_dataset['labels']\n",
    "         \n",
    "        if self.config[\"fisher\"]:\n",
    "            self.cluster = learn_gmm(self.train_features_cluster, n_modes=self.config[\"n_words\"], gm_args={'n_init': 1, 'max_iter': 50, 'covariance_type':'diag'})\n",
    "        else:\n",
    "            self.cluster = MiniBatchKMeans(n_clusters=self.config['n_words'], n_init='auto', compute_labels=False, random_state=123)\n",
    "            self.cluster.fit(self.train_features_cluster)\n",
    "        \n",
    "        dim_des = 128 if self.config['descriptor'] != 'akaze' else 64\n",
    "        dim_size = self.config['n_words'] if not self.config[\"fisher\"] else 2*self.config['n_words']*dim_des + self.config['n_words'] # n_words or 2KD+K for fisher\n",
    "        visual_words_train = np.zeros((len(train_features_level), self.config['level_pyramid'], dim_size), dtype=np.float64) # we will need shape (n_images, dim_size)\n",
    "        \n",
    "        for i in range(len(train_features_level)):\n",
    "            for l in range(self.config['level_pyramid']):\n",
    "                words = self.cluster.predict(train_features_level[i][l]) if not self.config[\"fisher\"] else fisher_vector(train_features_level[i][l], self.cluster)\n",
    "                visual_words_train[i,l,:] = np.bincount(words, minlength=self.config['n_words']) if not self.config[\"fisher\"] else words\n",
    "\n",
    "\n",
    "        visual_words_train = visual_words_train.reshape(len(train_features_level), -1) #get shape (n_images, dim_size)\n",
    "\n",
    "        if self.config['scaler']:\n",
    "            visual_words_train = self.scaler.fit_transform(visual_words_train)\n",
    "\n",
    "        if self.dim_red is not None:\n",
    "            visual_words_train = self.dim_red.fit_transform(visual_words_train)\n",
    "        \n",
    "         # Compute distance/kenrel matrix\n",
    "        if self.config['classifier'] == 'knn' and self.config['metric'] == 'precomputed':\n",
    "            self.visual_words_train_old = visual_words_train.copy()\n",
    "            visual_words_train = histogram_intersection_distance(visual_words_train, visual_words_train)\n",
    "        elif self.config['classifier'] == 'svm' and self.config['kernel'] == 'precomputed':\n",
    "            self.visual_words_train_old = visual_words_train.copy()\n",
    "            visual_words_train = histogram_intersection_kernel(visual_words_train, visual_words_train)\n",
    "            \n",
    "        self.classifier.fit(visual_words_train, y_train_labels)\n",
    "\n",
    "\n",
    "    def predict(self, test_features_level=None):\n",
    "\n",
    "        if test_features_level is None:\n",
    "            test_features_level = self.test_features_level\n",
    "\n",
    "        dim_des = 128 if self.config['descriptor'] != 'akaze' else 64\n",
    "        dim_size = self.config['n_words'] if not self.config[\"fisher\"] else 2*self.config['n_words']*dim_des + self.config['n_words'] # n_words or 2KD+K for fisher\n",
    "        visual_words_test = np.zeros((len(test_features_level), self.config['level_pyramid'], dim_size), dtype=np.float64) # we will need shape (n_images, dim_size)\n",
    "\n",
    "\n",
    "        for i in range(len(test_features_level)):\n",
    "            for l in range(self.config['level_pyramid']):\n",
    "                words = self.cluster.predict(test_features_level[i][l]) if not self.config[\"fisher\"] else fisher_vector(test_features_level[i][l], self.cluster)\n",
    "                visual_words_test[i,l,:] = np.bincount(words, minlength=self.config['n_words']) if not self.config[\"fisher\"] else words\n",
    "\n",
    "        visual_words_test = visual_words_test.reshape(len(test_features_level), -1) # get shape (n_images, dim_size*level_pyramid)\n",
    "\n",
    "        if self.config['scaler']:\n",
    "            visual_words_test = self.scaler.transform(visual_words_test)\n",
    "        \n",
    "        if self.dim_red is not None:\n",
    "            visual_words_test = self.dim_red.transform(visual_words_test)\n",
    "\n",
    "        # Compute distance/kenrel matrix\n",
    "        if self.config['classifier'] == 'knn' and self.config['metric'] == 'precomputed':\n",
    "            visual_words_test = histogram_intersection_distance(visual_words_test, self.visual_words_train_old)\n",
    "        elif self.config['classifier'] == 'svm' and self.config['kernel'] == 'precomputed':\n",
    "            visual_words_test = histogram_intersection_kernel(visual_words_test, self.visual_words_train_old)\n",
    "\n",
    "        return self.classifier.predict(visual_words_test), self.classifier.predict_proba(visual_words_test)\n",
    "    \n",
    "def cross_validation(bovw):\n",
    "    \n",
    "    kf = KFold(n_splits=bovw.config['n_folds'], shuffle=True, random_state=123)\n",
    "    predictions, y_scores, labels = [], [], []\n",
    "\n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(bovw.train_features_level), desc='Cross validation', total=bovw.config['n_folds']):\n",
    "        # Split features and labels for this fold\n",
    "        X_train_level, X_test_level = [bovw.train_features_level[i] for i in train_index] , [bovw.train_features_level[i] for i in test_index]\n",
    "        y_train_level, y_test = bovw.train_dataset['labels'][train_index], bovw.train_dataset['labels'][test_index]\n",
    "\n",
    "        X_train_cluster = np.concatenate([np.concatenate(image_feat_array, axis=0) for image_feat_array in X_train_level], axis=0)\n",
    "\n",
    "        # Fit the classifier\n",
    "        bovw.fit(X_train_cluster, X_train_level, y_train_level)\n",
    "        # Predict the test set\n",
    "        pred, y_score = bovw.predict(X_test_level)\n",
    "        \n",
    "        y_scores.append(y_score)\n",
    "        predictions.append(pred)\n",
    "        labels.append(y_test)\n",
    "\n",
    "    return predictions, y_scores, labels\n",
    "    \n",
    "\n",
    "def log_metrics(config, predictions, y_scores, y_test):\n",
    "    wandb.log({\n",
    "        'accuracy': np.mean([accuracy(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "        'average_precision': np.mean([average_precision(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "        'average_recall': np.mean([average_recall(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "        'average_f1': np.mean([average_f1(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "    })\n",
    "\n",
    "    # ----------------- #\n",
    "    #Log ROC curve\n",
    "    n_folds = config['n_folds']\n",
    "    result_roc = np.empty((n_folds, 1000))\n",
    "    result_auc = []\n",
    "\n",
    "    for n in range(n_folds):\n",
    "        y_onehot_test = LabelBinarizer().fit_transform(y_test[n])\n",
    "        fpr_grid, mean_tpr = compute_macro_roc_curve(y_onehot_test, y_scores[n])\n",
    "\n",
    "        #Average it and compute AUC\n",
    "        result_roc[n] = mean_tpr \n",
    "        result_auc.append(auc(fpr_grid, mean_tpr))\n",
    "\n",
    "    wandb.log({'AUC': np.mean(result_auc)})\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    # plt.plot(\n",
    "    #     fpr_grid,\n",
    "    #     result_roc.mean(axis=0),\n",
    "    #     label=f\"macro-average ROC curve (AUC = {np.mean(result_auc):.2f})\",\n",
    "    #     color=\"tab:blue\",\n",
    "    #     linestyle=\":\",\n",
    "    #     linewidth=4,\n",
    "    # )\n",
    "\n",
    "    # # plot chance level\n",
    "    # plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", linewidth=2, label=\"Chance\")\n",
    "\n",
    "    # plt.axis(\"square\")\n",
    "    # plt.xlabel(\"False Positive Rate\")\n",
    "    # plt.ylabel(\"True Positive Rate\")\n",
    "    # plt.legend()\n",
    "    # wandb.log({f\"Mean {n_folds}Fold-CV Macro-averaged One-vs-Rest ROC\": plt})\n",
    "\n",
    "\n",
    "def sweep_bovw():\n",
    "    with wandb.init() as run:\n",
    "        # Get hyperparameters\n",
    "        config = run.config\n",
    "        if config['fisher']:\n",
    "            config['n_words'] = config['n_words'] // 2\n",
    "        print(config)\n",
    "        bovw = BoVW(config)\n",
    "        predictions, y_scores, labels = cross_validation(bovw)\n",
    "        log_metrics(config, predictions, y_scores, labels)\n",
    "\n",
    "# config = {\n",
    "#     'descriptor': 'dense_sift',\n",
    "#     'step_size': 25,\n",
    "#     'n_words': 128,\n",
    "#     \"n_features\": 1024,\n",
    "#     'n_neigh': 5,\n",
    "#     'metric': 'euclidean',\n",
    "#     'n_folds': 2,\n",
    "#     'n_components': 8,\n",
    "#     'level_pyramid': 3,\n",
    "#     'C':1,\n",
    "#     'degree_pol': 3,\n",
    "#     'kernel': 'rbf',\n",
    "#     'classifier': 'svm',\n",
    "#     'fisher': False,\n",
    "#     'scaler': True,\n",
    "# }\n",
    "# bovw = BoVW(config, size_per_class=10, folder_path_train='./train', folder_path_test='./test')\n",
    "# bovw.fit()\n",
    "# predictions = bovw.predict()\n",
    "# predictions, y_scores, labels = cross_validation(bovw)\n",
    "# log_metrics(config, predictions, y_scores, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hny0a3ht\n",
      "Sweep URL: https://wandb.ai/c3-mcv/bovw/sweeps/hny0a3ht\n"
     ]
    }
   ],
   "source": [
    "# 'n_neigh': {'distribution':'int_uniform', 'min': 1, 'max': 10}\n",
    "# 'metric': {'values': ['euclidean', 'manhattan', 'cosine', 'precomputed]} # implement histogram intersection function distance\n",
    "# 'step_size': {'distribution':'int_uniform', 'min': 4, 'max': 50},\n",
    "\n",
    "# Define the sweep for each classifier and each descriptor\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep-SVM-akaze\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"accuracy\"},\n",
    "    \"parameters\": {\n",
    "        'classifier': {'value': 'svm'},\n",
    "        'descriptor': {'value': 'akaze'},\n",
    "        'n_features': {'value': 1024},\n",
    "        'n_folds': {'value': 5},\n",
    "        'n_words': {'distribution':'int_uniform', 'min': 128, 'max': 512},\n",
    "        'level_pyramid': {'distribution':'int_uniform', 'min': 1, 'max': 4},\n",
    "        'n_components': {'distribution':'int_uniform', 'min': 0, 'max': 64},\n",
    "        'kernel': {'values': ['rbf', 'linear', 'poly', 'precomputed']},\n",
    "        'degree_pol': {'distribution':'int_uniform', 'min': 2, 'max': 5},\n",
    "        'C': {'values': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'fisher': {'values': [True, False]},\n",
    "        'scaler': {'values': [True, False]},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"bovw\", entity=\"c3-mcv\") # EJECUTAR UNA VEZ POR CLASIFICADOR Y DESCRIPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent('c3-mcv/bovw/mi49ie8c', function=sweep_bovw, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking imbalance\n",
    "# Create a bar plot counting the number of images per class\n",
    "colors = [\"tab:red\", \"tab:green\", \"tab:blue\", \"tab:brown\", \"tab:orange\", \"tab:purple\", \"tab:pink\", \"brown\"]\n",
    "\n",
    "unique, counts = np.unique(bovw.train_dataset['labels'], return_counts=True)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(unique, counts, color=colors)\n",
    "plt.title('Number of images per class train')\n",
    "plt.show()\n",
    "\n",
    "# Create a bar plot counting the number of images per class\n",
    "unique, counts = np.unique(bovw.test_dataset['labels'], return_counts=True)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(unique, counts, color=colors)\n",
    "plt.title('Number of images per class test')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
