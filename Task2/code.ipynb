{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Luis/.netrc\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import os, tqdm, pickle, cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, ConfusionMatrixDisplay, average_precision_score, precision_recall_curve, PrecisionRecallDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from skimage.feature import fisher_vector, learn_gmm\n",
    "# from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "from typing import *\n",
    "from utils import *\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_ENTITY\"] = \"c3-mcv\"\n",
    "wandb.login(key = '14a56ed86de5bf43e377d95d05458ca8f15f5017', relogin=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_intersection_kernel(X, Y):\n",
    "    \"\"\"\n",
    "    Histogram intersection kernel.\n",
    "    \n",
    "    Parameters:\n",
    "        X: array-like of shape (n_samples_X, n_features)\n",
    "        Y: array-like of shape (n_samples_Y, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        kernel_matrix: array of shape (n_samples_X, n_samples_Y)\n",
    "    \"\"\"\n",
    "    # Expand dimensions of X and Y for broadcasting\n",
    "    X_expanded = np.expand_dims(X, 1)\n",
    "    Y_expanded = np.expand_dims(Y, 0)\n",
    "\n",
    "    # Compute the minimum between each pair of vectors (broadcasting)\n",
    "    minima = np.minimum(X_expanded, Y_expanded)\n",
    "\n",
    "    # Sum over the feature dimension to compute the kernel\n",
    "    kernel_matrix = np.sum(minima, axis=2)\n",
    "\n",
    "    return kernel_matrix\n",
    "\n",
    "def histogram_intersection_distance(X, Y):\n",
    "    \"\"\"\n",
    "    Histogram intersection distance for kNN.\n",
    "    \n",
    "    Parameters:\n",
    "        X: array-like of shape (n_samples_X, n_features)\n",
    "        Y: array-like of shape (n_samples_Y, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        distance_matrix: array of shape (n_samples_X, n_samples_Y)\n",
    "    \"\"\"\n",
    "    # Calculate the histogram intersection similarity\n",
    "    similarity = histogram_intersection_kernel(X, Y)\n",
    "    \n",
    "    max_similarity = np.minimum(X.sum(axis=1)[:, np.newaxis], Y.sum(axis=1)[np.newaxis, :])\n",
    "    return 1 - (similarity / max_similarity)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a set of predictions compared to the actual labels.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted values.\n",
    "        labels: numpy array containing the actual labels.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the accuracy.\n",
    "    \"\"\"\n",
    "    return sum(predictions == labels) / len(labels)\n",
    "\n",
    "def precision(predictions, labels, class_label):\n",
    "    \"\"\"\n",
    "    Calculates precision for a specific class in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "        class_label: the specific class for which precision is calculated.\n",
    "\n",
    "    Returns:\n",
    "        Precision value for the specified class.\n",
    "    \"\"\"\n",
    "    tp = np.sum((predictions == class_label) & (labels == class_label))\n",
    "    fp = np.sum((predictions == class_label) & (labels != class_label))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def recall(predictions, labels, class_label):\n",
    "    \"\"\"\n",
    "    Calculates recall for a specific class in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "        class_label: the specific class for which recall is calculated.\n",
    "\n",
    "    Returns:\n",
    "        Recall value for the specified class.\n",
    "    \"\"\"\n",
    "    tp = np.sum((predictions == class_label) & (labels == class_label))\n",
    "    fn = np.sum((predictions != class_label) & (labels == class_label))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def average_precision(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the average precision across all classes in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        The average precision across all classes.\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    return np.mean([precision(predictions, labels, c) for c in classes])\n",
    "\n",
    "def average_recall(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the average recall across all classes in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        The average recall across all classes.\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    return np.mean([recall(predictions, labels, c) for c in classes])\n",
    "\n",
    "def average_f1(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the average F1 score across all classes in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        The average F1 score across all classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 2 * average_precision(predictions, labels) * average_recall(predictions, labels) / (average_precision(predictions, labels) + average_recall(predictions, labels))\n",
    "\n",
    "def compute_macro_roc_curve(y_onehot_test, y_score):\n",
    "    \"\"\"\n",
    "    Computes the ROC curve and ROC area for each class.\n",
    "    \n",
    "    Parameters:\n",
    "        y_onehot_test: array-like of shape (n_samples, n_classes)\n",
    "        prob_matrix: array-like of shape (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        fpr_grid: Array of false positive rates at which ROC curves are evaluated.\n",
    "        mean_tpr: Array of mean true positive rates corresponding to the fpr_grid.\n",
    "    \"\"\"\n",
    "    n_classes = y_onehot_test.shape[1]\n",
    "    # store the fpr, tpr\n",
    "    fpr, tpr = dict(), dict()\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    return fpr_grid, mean_tpr / n_classes\n",
    "\n",
    "def compute_micro_roc_curve(y_onehot_test, y_score):\n",
    "    \"\"\"\n",
    "    Computes the micro ROC curve and ROC area.\n",
    "    \n",
    "    Parameters:\n",
    "        y_onehot_test: array-like of shape (n_samples, n_classes)\n",
    "        prob_matrix: array-like of shape (n_samples, n_classes)\n",
    "        \n",
    "    Returns:\n",
    "        fpr: Array of false positive rates.\n",
    "        tpr: Array of true positive rates.\n",
    "    \"\"\"\n",
    "    # Compute micro-average ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\n",
    "\n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BoVW():\n",
    "    def __init__(self, config, model_path, size_per_class=1e9, data_path='./MIT_split'):\n",
    "        \"\"\"\n",
    "        Bag-of-Visual-Words (BoVW) image classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - config: Dictionary containing configuration parameters for the BoVW model.\n",
    "        - size_per_class: Maximum number of images per class to use for training.\n",
    "        - data_path: Path to the dataset folder.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.size_per_class = size_per_class\n",
    "        self.data_path = data_path\n",
    "        self._initialize_datasets()\n",
    "\n",
    "        # Compute features for each split\n",
    "        self.train_features = self._compute_features(self.train_dataset_blocks['image_paths'], 'train', model_path)\n",
    "        self.test_features = self._compute_features(self.test_dataset_blocks['image_paths'], 'test', model_path)\n",
    "\n",
    "        # Classification\n",
    "        if self.config['classifier'] == 'knn':\n",
    "            self.classifier = KNeighborsClassifier(n_neighbors=self.config['n_neigh'], n_jobs=-1, metric=self.config['metric'])\n",
    "        elif self.config['classifier'] == 'svm':\n",
    "            self.classifier = SVC(kernel = self.config['kernel'], class_weight = 'balanced', gamma = 'auto', C = self.config['C'], probability=True, random_state=123)\n",
    "        elif self.config['classifier'] == 'logistic':\n",
    "            self.classifier = LogisticRegression(multi_class = 'auto', penalty='l2', max_iter=300, solver='lbfgs', C = self.config['C'], class_weight = 'balanced', n_jobs=-1, random_state=123)\n",
    "        \n",
    "        # Dimensionality reduction\n",
    "        self.dim_red = None\n",
    "        if self.config['n_components'] > 0:\n",
    "            self.dim_red = PCA(n_components = self.config['n_components'])\n",
    "\n",
    "        # Standarization\n",
    "        self.scaler = None\n",
    "        if self.config['scaler']:\n",
    "            self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    \n",
    "    def _compute_features(self, dataset, type_dataset, model_path):\n",
    "        \"\"\"\n",
    "        Computes the features for the train and test splits using dense SIFT.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.config['features'] == 'mlp':\n",
    "\n",
    "            mlp = build_mlp(in_size=self.config['patch_size'], out_size=self.config['out_size'], num_layers=self.config['num_layers'], activation=self.config['activation'], phase='train')\n",
    "\n",
    "            if not os.path.exists(model_path) and type_dataset == 'train':\n",
    "                block_path = self.data_path + f'_{self.config[\"patch_size\"]}_blocks_{self.config[\"overlap\"]}_overlap/train'\n",
    "                train(block_path, mlp, model_path, self.config)\n",
    "            \n",
    "            elif not os.path.exists(model_path) and type_dataset == 'test':\n",
    "                raise Exception('Model not found. Please train the model first.')\n",
    "            \n",
    "            mlp.load_weights(model_path)\n",
    "            self.mlp = keras.Model(inputs=mlp.input, outputs=mlp.get_layer('output').output)\n",
    "\n",
    "        elif self.config['features'] == 'dense_sift':\n",
    "            # Initialize Dense SIFT extractor\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp = [cv2.KeyPoint(x, y, self.config['kp_scale']) for y in range(0, self.config['patch_size'], self.config['step_size'])\n",
    "                                                              for x in range(0, self.config['patch_size'], self.config['step_size'])]\n",
    "        \n",
    "        # Initialize features matrix\n",
    "        n = self.n_images_train if type_dataset == 'train' else self.n_images_test\n",
    "        n_features = len(kp) if self.config['features'] == 'dense_sift' else 1\n",
    "        feat_dim = 128 if self.config['features'] == 'dense_sift' else self.config['out_size']\n",
    "        features = np.empty((n, self.n_patches_per_image, n_features, feat_dim))\n",
    "\n",
    "        idx_img = -1\n",
    "        batch = []\n",
    "        # Compute features for each image\n",
    "        for j, filename in tqdm.tqdm(enumerate(dataset), desc=f'Extracting features from {type_dataset} dataset', total=len(dataset)):\n",
    "            if j % self.n_patches_per_image == 0:\n",
    "                idx_img += 1\n",
    "\n",
    "            # Load image\n",
    "            img = cv2.imread(filename)\n",
    "            color = cv2.COLOR_BGR2GRAY if self.config['features'] == 'dense_sift' else cv2.COLOR_BGR2RGB\n",
    "            img = cv2.cvtColor(img, color)\n",
    "            \n",
    "            # Compute descriptors\n",
    "            if self.config['features'] == 'dense_sift':\n",
    "                _, des = sift.compute(img, kp) # shape (n_kp, 128)\n",
    "                features[idx_img, j % self.n_patches_per_image, :, :] = des\n",
    "            elif self.config['features'] == 'mlp':\n",
    "                batch.append(img)\n",
    "            \n",
    "        if self.config['features'] == 'mlp':\n",
    "            batch = np.array(batch)\n",
    "            des = self.mlp.predict(batch / 255, verbose=1)\n",
    "            features = des.reshape(n, self.n_patches_per_image, 1, -1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _create_directory(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _extract_patches(self, image_path, save_path, dataset_blocks, steps):\n",
    "        \"\"\"\n",
    "        Splits an image into patches.\n",
    "\n",
    "        :param image_path: Path to the input image.\n",
    "        :param destination_path: Path where the patches will be saved.\n",
    "        :param dataset_blocks: Dictionary containing the paths to the patches and their corresponding labels.\n",
    "        :param steps: Number of steps to move the sliding window.\n",
    "        \"\"\"\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        i = 0\n",
    "        # Extract and save patches\n",
    "        for x in steps:\n",
    "            for y in steps:\n",
    "                patch_path = os.path.join(save_path, f\"{os.path.splitext(os.path.basename(image_path))[0]}_{i}.jpg\")\n",
    "                box = (x, y, x + self.config['patch_size'], y + self.config['patch_size'])\n",
    "                image.crop(box).save(patch_path)\n",
    "                dataset_blocks['image_paths'].append(patch_path)\n",
    "                dataset_blocks['labels'].append(os.path.basename(save_path))\n",
    "                i += 1\n",
    "        \n",
    "        return dataset_blocks\n",
    "\n",
    "    def _process_split(self, split, block_path):\n",
    "        dataset = {'image_paths': [], 'labels': []}\n",
    "        dataset_blocks = {'image_paths': [], 'labels': []}\n",
    "        split_path = os.path.join(self.data_path, split)\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        steps = range(0, 256 - self.config['patch_size'] + 1, self.config['patch_size'] - self.config['patch_size']//self.config['overlap'])\n",
    "        self.n_patches_per_image = len(steps)**2\n",
    "\n",
    "        for label in tqdm.tqdm(os.listdir(split_path), desc=f'Creating {split} patches...'):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            block_path_exists = self._create_directory(os.path.join(block_path, split, label))\n",
    "\n",
    "            for i, image_name in enumerate(os.listdir(label_path)):\n",
    "                if i >= self.size_per_class:\n",
    "                    break\n",
    "\n",
    "                image_path = os.path.join(label_path, image_name)\n",
    "                dataset['image_paths'].append(image_path)\n",
    "                dataset['labels'].append(label)\n",
    "\n",
    "                if not block_path_exists:\n",
    "                    dataset_blocks = self._extract_patches(image_path, os.path.join(block_path, split, label), dataset_blocks, steps)\n",
    "            \n",
    "            if block_path_exists:\n",
    "                paths = [os.path.join(os.path.join(block_path, split, label), path) for path in os.listdir(os.path.join(block_path, split, label))][:int(self.size_per_class)]\n",
    "                dataset_blocks['image_paths'].extend(paths)\n",
    "                dataset_blocks['labels'].extend([label] * len(paths))\n",
    "\n",
    "        dataset['labels'] = np.array(dataset['labels'])\n",
    "        dataset_blocks['labels'] = np.array(dataset_blocks['labels'])\n",
    "        return dataset, dataset_blocks\n",
    "\n",
    "    def _initialize_datasets(self):\n",
    "        block_path = self.data_path + f'_{self.config[\"patch_size\"]}_blocks_{self.config[\"overlap\"]}_overlap'\n",
    "        _ = self._create_directory(block_path)\n",
    "        self.train_dataset, self.train_dataset_blocks = self._process_split('train', block_path)\n",
    "        self.test_dataset, self.test_dataset_blocks = self._process_split('test', block_path)\n",
    "\n",
    "        self.n_images_train = len(self.train_dataset['image_paths'])\n",
    "        self.n_images_test = len(self.test_dataset['image_paths'])\n",
    "\n",
    "    def fit(self, train_features=None, y_train_labels=None):\n",
    "        \"\"\"\n",
    "        Fit the Bag of Visual Words (BoVW) model using training data.\n",
    "\n",
    "        Parameters:\n",
    "        \"\"\"\n",
    "        \n",
    "        if train_features is None and y_train_labels is None:\n",
    "            train_features = self.train_features\n",
    "            y_train_labels = self.train_dataset['labels']\n",
    "        \n",
    "\n",
    "        n, _, _, dim_des = train_features.shape #n_images, n_patches_per_image, n_features, feat_dim\n",
    "        reshaped_features = train_features.reshape(-1,dim_des)\n",
    "            \n",
    "        # Clustering for the visual words\n",
    "        if self.config[\"fisher\"]:\n",
    "            print('Fitting GMM fisher...')\n",
    "            self.cluster = learn_gmm(reshaped_features, n_modes=self.config[\"n_words\"], gm_args={'n_init': 1, 'max_iter': 50, 'covariance_type':'diag'})       \n",
    "        else:\n",
    "            print('Fitting Kmeans clustering...')\n",
    "            self.cluster = MiniBatchKMeans(n_clusters=self.config['n_words'], n_init='auto', compute_labels=False, random_state=123)\n",
    "            self.cluster.fit(reshaped_features)\n",
    "        \n",
    "        # Calculate the visual words for each image and level\n",
    "        dim_size = self.config['n_words'] if not self.config[\"fisher\"] else 2*self.config['n_words']*dim_des + self.config['n_words'] # n_words or 2KD+K for fisher\n",
    "\n",
    "        # Predict the cluster labels for all patches at once\n",
    "        print('Creating train visual words...')\n",
    "        if not self.config[\"fisher\"]:\n",
    "            all_words = self.cluster.predict(reshaped_features).reshape(n, self.n_patches_per_image, -1)\n",
    "            visual_words_train = np.apply_along_axis(lambda x: np.bincount(x, minlength=self.config['n_words']).astype(np.float64), axis=2, arr=all_words)\n",
    "            visual_words_train /= np.sum(visual_words_train, axis=2, keepdims=True)\n",
    "        else:\n",
    "            visual_words_train = fisher_vector(reshaped_features, self.cluster)\n",
    "\n",
    "        # Standarization\n",
    "        if self.config['scaler']:\n",
    "            self.means, self.sigmas = np.empty((self.n_patches_per_image, dim_size)), np.empty((self.n_patches_per_image, dim_size))\n",
    "            for patch in range(self.n_patches_per_image):\n",
    "                visual_words_train[:,patch,:] = self.scaler.fit_transform(visual_words_train[:,patch,:])\n",
    "                self.means[patch] = self.scaler.mean_\n",
    "                self.sigmas[patch] = np.sqrt(self.scaler.var_)\n",
    "\n",
    "        visual_words_train = visual_words_train.reshape(n, -1) #concatenate patches and get shape (n_images, dim_size)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        if self.dim_red is not None:\n",
    "            visual_words_train = self.dim_red.fit_transform(visual_words_train)\n",
    "        \n",
    "        # Compute distance/kenrel matrix\n",
    "        if self.config['classifier'] == 'knn' and self.config['metric'] == 'precomputed':\n",
    "            self.visual_words_train_old = visual_words_train.copy()\n",
    "            visual_words_train = histogram_intersection_distance(visual_words_train, visual_words_train)\n",
    "        elif self.config['classifier'] == 'svm' and self.config['kernel'] == 'precomputed':\n",
    "            self.visual_words_train_old = visual_words_train.copy()\n",
    "            visual_words_train = histogram_intersection_kernel(visual_words_train, visual_words_train)\n",
    "            \n",
    "        # Fit the classifier\n",
    "        self.classifier.fit(visual_words_train, y_train_labels)\n",
    "\n",
    "    def predict(self, test_features=None):\n",
    "        \"\"\"\n",
    "        Predict labels for test data using the trained BoVW model.\n",
    "\n",
    "        Parameters:\n",
    "            test_features_level (array-like, optional): Detailed features at different scales within each test image. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Predicted labels and class probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        if test_features is None:\n",
    "            test_features = self.test_features\n",
    "\n",
    "        n, _, _, dim_des = test_features.shape #n_images, n_patches_per_image, n_features, feat_dim\n",
    "        reshaped_features = test_features.reshape(-1,dim_des)\n",
    "\n",
    "        # Calculate the visual words for each image and level\n",
    "        dim_size = self.config['n_words'] if not self.config[\"fisher\"] else 2*self.config['n_words']*dim_des + self.config['n_words'] # n_words or 2KD+K for fisher\n",
    "        print('Creating test visual words...')\n",
    "        if not self.config[\"fisher\"]:\n",
    "            all_words = self.cluster.predict(reshaped_features).reshape(n, self.n_patches_per_image, -1)\n",
    "            visual_words_test = np.apply_along_axis(lambda x: np.bincount(x, minlength=self.config['n_words']).astype(np.float64), axis=2, arr=all_words)\n",
    "            visual_words_test /= np.sum(visual_words_test, axis=2, keepdims=True)\n",
    "        else:\n",
    "            visual_words_test = fisher_vector(reshaped_features, self.cluster)\n",
    "\n",
    "        # Standarization\n",
    "        if self.config['scaler']:\n",
    "            for patch in range(self.n_patches_per_image):\n",
    "                visual_words_test[:,patch,:] = (visual_words_test[:,patch,:] - self.means[patch]) / (self.sigmas[patch] + 1e-6)\n",
    "\n",
    "        visual_words_test = visual_words_test.reshape(n, -1) #concatenate patches and get shape (n_images, dim_size)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        if self.dim_red is not None:\n",
    "            visual_words_test = self.dim_red.transform(visual_words_test)\n",
    "\n",
    "        # Compute distance/kenrel matrix\n",
    "        if self.config['classifier'] == 'knn' and self.config['metric'] == 'precomputed':\n",
    "            visual_words_test = histogram_intersection_distance(visual_words_test, self.visual_words_train_old)\n",
    "        elif self.config['classifier'] == 'svm' and self.config['kernel'] == 'precomputed':\n",
    "            visual_words_test = histogram_intersection_kernel(visual_words_test, self.visual_words_train_old)\n",
    "\n",
    "        # Predict labels and class probabilities\n",
    "        return self.classifier.predict(visual_words_test), self.classifier.predict_proba(visual_words_test)\n",
    "    \n",
    "def cross_validation(bovw):\n",
    "    \"\"\"\n",
    "    Perform cross-validation using the given BoVW model.\n",
    "\n",
    "    Parameters:\n",
    "        bovw (BoVW): Bag of Visual Words model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Predictions, class probabilities, and true labels for each cross-validation fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits=bovw.config['n_folds'], shuffle=True, random_state=123)\n",
    "    predictions, y_scores, labels = [], [], []\n",
    "\n",
    "    _, n_p, _, _ = bovw.train_features.shape\n",
    "\n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(bovw.train_features), desc='Cross validation', total=bovw.config['n_folds']):\n",
    "        # Split features and labels for this fold\n",
    "        X_train, X_test = bovw.train_features[train_index], bovw.train_features[test_index]\n",
    "        y_train, y_test = bovw.train_dataset['labels'][train_index], bovw.train_dataset['labels'][test_index]\n",
    "\n",
    "        # Fit the classifier\n",
    "        bovw.fit(X_train, y_train)\n",
    "        # Predict the test set\n",
    "        pred, y_score = bovw.predict(X_test)\n",
    "        \n",
    "        y_scores.append(y_score)\n",
    "        predictions.append(pred)\n",
    "        labels.append(y_test)\n",
    "\n",
    "    return predictions, y_scores, labels\n",
    "    \n",
    "def log_metrics(config, predictions, y_score, y_test):\n",
    "    \"\"\"\n",
    "    Logs various evaluation metrics and PR curve data to WandB.\n",
    "\n",
    "    Parameters:\n",
    "        config (dict): Configuration parameters for the model.\n",
    "        predictions (list): List of predicted labels for each fold in cross-validation.\n",
    "        y_scores (list): List of predicted probabilities for each fold in cross-validation.\n",
    "        y_test (list): List of true labels for each fold in cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    wandb.log({\n",
    "        'accuracy': np.mean([accuracy(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "        'average_precision': np.mean([average_precision(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "        'average_recall': np.mean([average_recall(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "        'average_f1': np.mean([average_f1(predictions[i], y_test[i]) for i in range(len(predictions))]),\n",
    "    })\n",
    "\n",
    "    # ----------------- #\n",
    "    n_folds = config['n_folds']\n",
    "    result_auc = []\n",
    "    for n in range(n_folds):\n",
    "        y_onehot_test = LabelBinarizer().fit_transform(y_test[n])\n",
    "        #compute AUC\n",
    "        result_auc.append(average_precision_score(y_onehot_test, y_score[n], average=\"micro\"))\n",
    "    \n",
    "    wandb.log({'AUC': np.mean(result_auc)})\n",
    "\n",
    "\n",
    "def sweep_bovw():\n",
    "    with wandb.init() as run:\n",
    "        # Get hyperparameters\n",
    "        config = run.config\n",
    "        if config['fisher']:\n",
    "            config['n_words'] = config['n_words'] // 2\n",
    "        print(config)\n",
    "        model_path = f'pretrained/model_weights_{config[\"patch_size\"]}_{config[\"overlap\"]}_{config[\"out_size\"]}_{config[\"num_layers\"]}_{config[\"activation\"]}.h5'\n",
    "        bovw = MLP_BoVW(config, model_path)\n",
    "        predictions, y_scores, labels = cross_validation(bovw)\n",
    "        log_metrics(predictions, y_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train patches...: 100%|██████████| 8/8 [00:27<00:00,  3.40s/it]\n",
      "Creating test patches...: 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]\n",
      "Extracting features from train dataset: 100%|██████████| 120384/120384 [09:28<00:00, 211.84it/s]\n",
      "Extracting features from test dataset: 100%|██████████| 51648/51648 [03:51<00:00, 223.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Kmeans clustering...\n",
      "Creating train visual words...\n",
      "Creating test visual words...\n",
      "0.7472118959107806\n",
      "0.7490912860639142\n",
      "0.7478668633925184\n",
      "0.7484785739757809\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'patch_size': 16,\n",
    "    'overlap': 16, # overlap proportion of patches -> 4 means 1/4 of the patch is overlapped\n",
    "    'step_size': 8, # distance between dense sift keypoints\n",
    "    'kp_scale': 8, # size of dense sift keypoints\n",
    "    'out_size': 512, # size of the output of the mlp\n",
    "    'num_layers': 1, # number of layers of the mlp\n",
    "    'activation': 'linear', # activation function of the mlp\n",
    "    'features': 'mlp', # dense_sift or mlp\n",
    "    'classifier': 'svm',\n",
    "    'n_words': 64,\n",
    "    'scaler': True,\n",
    "    'fisher': False,\n",
    "    'n_components': 32,\n",
    "    'kernel': 'precomputed',\n",
    "    'C': 0.01,\n",
    "    'n_folds': 5,\n",
    "}\n",
    "\n",
    "model_path = f'pretrained/model_weights_{config[\"patch_size\"]}_{config[\"out_size\"]}_{config[\"num_layers\"]}_{config[\"activation\"]}.h5'\n",
    "bovw = MLP_BoVW(config, model_path)\n",
    "bovw.fit()\n",
    "predictions, y_scores = bovw.predict()\n",
    "print(accuracy(predictions, bovw.test_dataset['labels']))\n",
    "print(average_precision(predictions, bovw.test_dataset['labels']))\n",
    "print(average_recall(predictions, bovw.test_dataset['labels']))\n",
    "print(average_f1(predictions, bovw.test_dataset['labels']))\n",
    "# predictions, y_scores, labels = cross_validation(bovw)\n",
    "# log_metrics(config, predictions, y_scores, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: au5ybwnk\n",
      "Sweep URL: https://wandb.ai/c3-mcv/bovw-mlp/sweeps/au5ybwnk\n"
     ]
    }
   ],
   "source": [
    "# Define the sweep for each classifier and each descriptor\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep-svm-mlp\",\n",
    "    \"program\": 'code.py',\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"accuracy\"},\n",
    "    \"parameters\": {\n",
    "        'classifier': {'value': 'svm'},\n",
    "        'descriptor': {'value': 'mlp'},\n",
    "        'n_folds': {'value': 5},\n",
    "        'n_words': {'values': [64,256,512,1024]},\n",
    "        'n_components': {'values': [8,16,32,64]},\n",
    "        'C': {'value': 0.01},\n",
    "        'fisher': {'value': False},\n",
    "        'scaler': {'value': True},\n",
    "        'kernel':  {'value': 'precomputed'},\n",
    "        'patch_size': {'values': [16,32,64,128]},\n",
    "        'overlap': {'values': [10,16]},\n",
    "        'out_size': {'values': [256,512,1024,2048]},\n",
    "        'num_layers': {'values': [2,4,8]},\n",
    "        'activation': {'values': ['relu','tanh']},\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"bovw-mlp\", entity=\"c3-mcv\") # EJECUTAR UNA VEZ POR CLASIFICADOR Y DESCRIPTOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
