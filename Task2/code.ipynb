{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Luis/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import os, tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.feature import fisher_vector, learn_gmm\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_ENTITY\"] = \"c3-mcv\"\n",
    "wandb.login(key = '14a56ed86de5bf43e377d95d05458ca8f15f5017', relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Input\n",
    "def build_mlp(input_size, phase='train'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_size, input_size, 3,), name='input'))\n",
    "    model.add(Reshape((input_size*input_size*3,)))\n",
    "    model.add(Dense(units=2048, activation='relu'))\n",
    "\n",
    "    # In the feature extractor phase, stop building the model before the last layer\n",
    "    if phase == 'feature_extractor':\n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        model.add(Dense(units=8, activation='linear' if phase == 'test' else 'softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train patches...: 100%|██████████| 8/8 [00:11<00:00,  1.41s/it]\n",
      "Creating test patches...: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it]\n",
      "Extracting features from dataset 0: 100%|██████████| 3600/3600 [00:53<00:00, 67.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 17s 149ms/step\n"
     ]
    }
   ],
   "source": [
    "class MLP_BoVW():\n",
    "    def __init__(self, config, size_per_class=1e9, data_path='./MIT_split', model_path = './models/mlp.h5'):\n",
    "        \"\"\"\n",
    "        Bag-of-Visual-Words (BoVW) image classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - config: Dictionary containing configuration parameters for the BoVW model.\n",
    "        - size_per_class: Maximum number of images per class to use for training.\n",
    "        - data_path: Path to the dataset folder.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.data_path = data_path\n",
    "        self.size_per_class = size_per_class\n",
    "        self._initialize_datasets()\n",
    "\n",
    "        # Compute features for each split\n",
    "        if self.config['features'] == 'mlp':  \n",
    "            self.train_features, self.test_features = self._compute_features_mlp(model_path)\n",
    "        elif self.config['features'] == 'dense_sift':\n",
    "            self.train_features, self.test_features = self._compute_features_dense_sift()\n",
    "\n",
    "        # Classification\n",
    "        if self.config['classifier'] == 'knn':\n",
    "            self.classifier = KNeighborsClassifier(n_neighbors=self.config['n_neigh'], n_jobs=-1, metric=self.config['metric'])\n",
    "        elif self.config['classifier'] == 'svm':\n",
    "            self.classifier = SVC(kernel = self.config['kernel'], degree=self.config['degree_pol'], class_weight = 'balanced', gamma = 'auto', C = self.config['C'], probability=True, random_state=123)\n",
    "        elif self.config['classifier'] == 'logistic':\n",
    "            self.classifier = LogisticRegression(multi_class = 'auto', penalty='l2', max_iter=300, solver='lbfgs', C = self.config['C'], class_weight = 'balanced', n_jobs=-1, random_state=123)\n",
    "        \n",
    "        # Dimensionality reduction\n",
    "        self.dim_red = None\n",
    "        if self.config['n_components'] > 0:\n",
    "            self.dim_red = PCA(n_components = self.config['n_components'])\n",
    "\n",
    "        # Standarization\n",
    "        self.scaler = None\n",
    "        if self.config['scaler']:\n",
    "            self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    def _compute_features_mlp(self, model_path = './models/mlp.h5'):\n",
    "        \"\"\"\n",
    "        Computes the features for the train and test splits using a MLP.\n",
    "        \"\"\"\n",
    "        model = build_mlp(input_size=self.config['patch_size'], phase='feature_extractor')\n",
    "        # model.load_weights(model_path)\n",
    "\n",
    "        for i,dataset in enumerate([self.train_dataset_blocks['image_paths'], self.test_dataset_blocks['image_paths']]):\n",
    "            batch = []\n",
    "            for filename in tqdm.tqdm(dataset, desc='Extracting features from dataset %d' % i):\n",
    "                batch.append(np.asarray(Image.open(filename)))\n",
    "            batch = np.array(batch)\n",
    "\n",
    "            if i == 0:\n",
    "                train_features = model.predict(batch)\n",
    "            else:\n",
    "                test_features = model.predict(batch)\n",
    "        \n",
    "        return train_features, test_features\n",
    "    \n",
    "    def _compute_features_dense_sift(self):\n",
    "        \"\"\"\n",
    "        Computes the features for the train and test splits using dense SIFT.\n",
    "        \"\"\"\n",
    "        \n",
    "        return train_features, test_features\n",
    "\n",
    "    def _create_directory(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    def _extract_patches(self, image_path, save_path, dataset_blocks, steps):\n",
    "        \"\"\"\n",
    "        Splits an image into patches.\n",
    "\n",
    "        :param image_path: Path to the input image.\n",
    "        :param destination_path: Path where the patches will be saved.\n",
    "        :param dataset_blocks: Dictionary containing the paths to the patches and their corresponding labels.\n",
    "        :param steps: Number of steps to move the sliding window.\n",
    "        \"\"\"\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        i = 0\n",
    "        # Extract and save patches\n",
    "        for x in steps:\n",
    "            for y in steps:\n",
    "                patch_path = os.path.join(save_path, f\"{os.path.splitext(os.path.basename(image_path))[0]}_{i}.jpg\")\n",
    "                box = (x, y, x + self.config['patch_size'], y + self.config['patch_size'])\n",
    "                image.crop(box).save(patch_path)\n",
    "                dataset_blocks['image_paths'].append(patch_path)\n",
    "                dataset_blocks['labels'].append(os.path.basename(save_path))\n",
    "                i += 1\n",
    "        \n",
    "\n",
    "    def _process_split(self, split, block_path):\n",
    "        dataset = {'image_paths': [], 'labels': []}\n",
    "        dataset_blocks = {'image_paths': [], 'labels': []}\n",
    "        split_path = os.path.join(self.data_path, split)\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        steps = range(0, 256 - self.config['patch_size'] + 1, self.config['patch_size'] - self.config['overlap'])\n",
    "\n",
    "        for label in tqdm.tqdm(os.listdir(split_path), desc=f'Creating {split} patches...'):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            self._create_directory(os.path.join(block_path, split, label))\n",
    "\n",
    "            for i, image_name in enumerate(os.listdir(label_path)):\n",
    "                if i >= self.size_per_class:\n",
    "                    break\n",
    "                image_path = os.path.join(label_path, image_name)\n",
    "                dataset['image_paths'].append(image_path)\n",
    "                dataset['labels'].append(label)\n",
    "\n",
    "                self._extract_patches(image_path, os.path.join(block_path, split, label), dataset_blocks, steps)\n",
    "\n",
    "        dataset['labels'] = np.array(dataset['labels'])\n",
    "        return dataset, dataset_blocks\n",
    "\n",
    "    def _initialize_datasets(self):\n",
    "        block_path = self.data_path + f'_{self.config[\"patch_size\"]}_blocks'\n",
    "        self._create_directory(block_path)\n",
    "        self.train_dataset, self.train_dataset_blocks = self._process_split('train', block_path)\n",
    "        self.test_dataset, self.test_dataset_blocks = self._process_split('test', block_path)\n",
    "\n",
    "config = {\n",
    "    'patch_size': 128,\n",
    "    'overlap': 64,\n",
    "    'features': 'mlp',\n",
    "    'classifier': 'logistic',\n",
    "    'scaler': True,\n",
    "    'n_components': 0,\n",
    "    'n_neigh': 1,\n",
    "    'metric': 'euclidean',\n",
    "    'kernel': 'linear',\n",
    "    'degree_pol': 3,\n",
    "    'C': 1,\n",
    "}\n",
    "bovw = MLP_BoVW(config, size_per_class=50, data_path='./MIT_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 2048)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bovw.train_features[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
