{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import os, tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.feature import fisher_vector, learn_gmm\n",
    "\n",
    "from typing import *\n",
    "\n",
    "# import wandb\n",
    "# os.environ[\"WANDB_ENTITY\"] = \"c3-mcv\"\n",
    "# wandb.login(key = '14a56ed86de5bf43e377d95d05458ca8f15f5017', relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Input\n",
    "def build_mlp(in_size, out_size, num_layers, activation, phase='train'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(in_size, in_size, 3,), name='input'))\n",
    "    model.add(Reshape((in_size*in_size*3,)))\n",
    "\n",
    "    if in_size*in_size*3 < out_size: \n",
    "        #increment size\n",
    "        size_step = (out_size - in_size*in_size*3) / (num_layers - 1)\n",
    "        sign = 1\n",
    "    else: \n",
    "        #decrement size \n",
    "        size_step = (in_size*in_size*3 - out_size) / (num_layers - 1)\n",
    "        sign = -1\n",
    "\n",
    "    # Add layers\n",
    "    for i in range(num_layers - 1):\n",
    "        layer_size = int(in_size*in_size*3 + sign * size_step * i)\n",
    "        model.add(Dense(units=layer_size, activation=activation))\n",
    "\n",
    "    model.add(Dense(units=out_size, activation=activation))\n",
    "\n",
    "    # In the feature extractor phase, stop building the model before the last layer\n",
    "    if phase == 'feature_extractor':\n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        model.add(Dense(units=8, activation='linear' if phase == 'test' else 'softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train patches...: 100%|██████████| 8/8 [01:15<00:00,  9.45s/it]\n",
      "Creating test patches...: 100%|██████████| 8/8 [00:13<00:00,  1.74s/it]\n",
      "Extracting features from train dataset: 100%|██████████| 30096/30096 [00:19<00:00, 1518.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30096, 64, 64, 3)\n",
      "941/941 [==============================] - 296s 314ms/step\n",
      "(30096, 2048)\n",
      "(1881, 16, 1, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from test dataset: 100%|██████████| 12912/12912 [00:07<00:00, 1692.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12912, 64, 64, 3)\n",
      "404/404 [==============================] - 128s 317ms/step\n",
      "(12912, 2048)\n",
      "(807, 16, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "class MLP_BoVW():\n",
    "    def __init__(self, config, size_per_class=1e9, data_path='./MIT_split', model_path = './models/mlp.h5'):\n",
    "        \"\"\"\n",
    "        Bag-of-Visual-Words (BoVW) image classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - config: Dictionary containing configuration parameters for the BoVW model.\n",
    "        - size_per_class: Maximum number of images per class to use for training.\n",
    "        - data_path: Path to the dataset folder.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.size_per_class = size_per_class\n",
    "        self.data_path = data_path\n",
    "        self._initialize_datasets()\n",
    "\n",
    "        # Compute features for each split\n",
    "        self.train_features = self._compute_features(self.train_dataset_blocks['image_paths'], 'train', model_path)\n",
    "        self.test_features = self._compute_features(self.test_dataset_blocks['image_paths'], 'test', model_path)\n",
    "\n",
    "        # Classification\n",
    "        if self.config['classifier'] == 'knn':\n",
    "            self.classifier = KNeighborsClassifier(n_neighbors=self.config['n_neigh'], n_jobs=-1, metric=self.config['metric'])\n",
    "        elif self.config['classifier'] == 'svm':\n",
    "            self.classifier = SVC(kernel = self.config['kernel'], degree=self.config['degree_pol'], class_weight = 'balanced', gamma = 'auto', C = self.config['C'], probability=True, random_state=123)\n",
    "        elif self.config['classifier'] == 'logistic':\n",
    "            self.classifier = LogisticRegression(multi_class = 'auto', penalty='l2', max_iter=300, solver='lbfgs', C = self.config['C'], class_weight = 'balanced', n_jobs=-1, random_state=123)\n",
    "        \n",
    "        # Dimensionality reduction\n",
    "        self.dim_red = None\n",
    "        if self.config['n_components'] > 0:\n",
    "            self.dim_red = PCA(n_components = self.config['n_components'])\n",
    "\n",
    "        # Standarization\n",
    "        self.scaler = None\n",
    "        if self.config['scaler']:\n",
    "            self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    \n",
    "    def _compute_features(self, dataset, type_dataset, model_path):\n",
    "        \"\"\"\n",
    "        Computes the features for the train and test splits using dense SIFT.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.config['features'] == 'mlp':\n",
    "            self.mlp = build_mlp(in_size=self.config['patch_size'], out_size=self.config['out_size'], num_layers=self.config['num_layers'], activation='relu', phase='feature_extractor')\n",
    "            # model.load_weights(model_path)\n",
    "        elif self.config['features'] == 'dense_sift':\n",
    "            # Initialize Dense SIFT extractor\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp = [cv2.KeyPoint(x, y, self.config['kp_scale']) for y in range(0, self.config['patch_size'], self.config['step_size'])\n",
    "                                                              for x in range(0, self.config['patch_size'], self.config['step_size'])]\n",
    "        \n",
    "        # Initialize features matrix\n",
    "        n = self.n_images_train if type_dataset == 'train' else self.n_images_test\n",
    "        n_features = len(kp) if self.config['features'] == 'dense_sift' else 1\n",
    "        feat_dim = 128 if self.config['features'] == 'dense_sift' else self.config['out_size']\n",
    "        features = np.empty((n, self.n_patches_per_image, n_features, feat_dim))\n",
    "\n",
    "        idx_img = -1\n",
    "        batch = []\n",
    "        # Compute features for each image\n",
    "        for j, filename in tqdm.tqdm(enumerate(dataset), desc=f'Extracting features from {type_dataset} dataset', total=len(dataset)):\n",
    "            if j % self.n_patches_per_image == 0:\n",
    "                idx_img += 1\n",
    "\n",
    "            # Load image\n",
    "            img = cv2.imread(filename)\n",
    "            color = cv2.COLOR_BGR2GRAY if self.config['features'] == 'dense_sift' else cv2.COLOR_BGR2RGB\n",
    "            img = cv2.cvtColor(img, color)\n",
    "            \n",
    "            # Compute descriptors\n",
    "            if self.config['features'] == 'dense_sift':\n",
    "                _, des = sift.compute(img, kp) # shape (n_kp, 128)\n",
    "                features[idx_img, j % self.n_patches_per_image, :, :] = des\n",
    "            elif self.config['features'] == 'mlp':\n",
    "                batch.append(img)\n",
    "            \n",
    "        if self.config['features'] == 'mlp':\n",
    "            batch = np.array(batch)\n",
    "            des = self.mlp.predict(batch, verbose=1)\n",
    "            des = des.reshape(n, self.n_patches_per_image, 1, -1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _create_directory(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _extract_patches(self, image_path, save_path, dataset_blocks, steps, block_path_exists):\n",
    "        \"\"\"\n",
    "        Splits an image into patches.\n",
    "\n",
    "        :param image_path: Path to the input image.\n",
    "        :param destination_path: Path where the patches will be saved.\n",
    "        :param dataset_blocks: Dictionary containing the paths to the patches and their corresponding labels.\n",
    "        :param steps: Number of steps to move the sliding window.\n",
    "        \"\"\"\n",
    "        if block_path_exists:\n",
    "            # get the paths that contains the image name in image path\n",
    "            paths = [os.path.join(save_path, path) for path in os.listdir(save_path) if path.startswith(os.path.basename(image_path).split('.')[0]+'_')]\n",
    "            dataset_blocks['image_paths'].extend(paths)\n",
    "            dataset_blocks['labels'].extend([os.path.basename(save_path)] * len(paths))\n",
    "            return dataset_blocks\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        i = 0\n",
    "        # Extract and save patches\n",
    "        for x in steps:\n",
    "            for y in steps:\n",
    "                patch_path = os.path.join(save_path, f\"{os.path.splitext(os.path.basename(image_path))[0]}_{i}.jpg\")\n",
    "                box = (x, y, x + self.config['patch_size'], y + self.config['patch_size'])\n",
    "                image.crop(box).save(patch_path)\n",
    "                dataset_blocks['image_paths'].append(patch_path)\n",
    "                dataset_blocks['labels'].append(os.path.basename(save_path))\n",
    "                i += 1\n",
    "        \n",
    "        return dataset_blocks\n",
    "\n",
    "    def _process_split(self, split, block_path):\n",
    "        dataset = {'image_paths': [], 'labels': []}\n",
    "        dataset_blocks = {'image_paths': [], 'labels': []}\n",
    "        split_path = os.path.join(self.data_path, split)\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        steps = range(0, 256 - self.config['patch_size'] + 1, self.config['patch_size'] - self.config['patch_size']//self.config['overlap'])\n",
    "        self.n_patches_per_image = len(steps)**2\n",
    "\n",
    "        for label in tqdm.tqdm(os.listdir(split_path), desc=f'Creating {split} patches...'):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            block_path_exists = self._create_directory(os.path.join(block_path, split, label))\n",
    "\n",
    "            for i, image_name in enumerate(os.listdir(label_path)):\n",
    "                if i >= self.size_per_class:\n",
    "                    break\n",
    "\n",
    "                image_path = os.path.join(label_path, image_name)\n",
    "                dataset['image_paths'].append(image_path)\n",
    "                dataset['labels'].append(label)\n",
    "\n",
    "                dataset_blocks = self._extract_patches(image_path, os.path.join(block_path, split, label), dataset_blocks, steps, block_path_exists)\n",
    "\n",
    "        dataset['labels'] = np.array(dataset['labels'])\n",
    "        return dataset, dataset_blocks\n",
    "\n",
    "    def _initialize_datasets(self):\n",
    "        block_path = self.data_path + f'_{self.config[\"patch_size\"]}_blocks_{self.config[\"overlap\"]}_overlap'\n",
    "        _ = self._create_directory(block_path)\n",
    "        self.train_dataset, self.train_dataset_blocks = self._process_split('train', block_path)\n",
    "        self.test_dataset, self.test_dataset_blocks = self._process_split('test', block_path)\n",
    "\n",
    "        self.n_images_train = len(self.train_dataset['image_paths'])\n",
    "        self.n_images_test = len(self.test_dataset['image_paths'])\n",
    "\n",
    "    def fit(self, train_features=None, y_train_labels=None):\n",
    "        \"\"\"\n",
    "        Fit the Bag of Visual Words (BoVW) model using training data.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if train_features is None and y_train_labels is None:\n",
    "            train_features = self.train_features\n",
    "            y_train_labels = self.train_dataset['labels']\n",
    "\n",
    "        n, _, _, dim_des = train_features.shape[-1] #n_images, n_patches_per_image, n_features, feat_dim\n",
    "            \n",
    "        # Clustering for the visual words\n",
    "        if self.config[\"fisher\"]:\n",
    "            self.cluster = learn_gmm(train_features.reshape(-1,dim_des), n_modes=self.config[\"n_words\"], gm_args={'n_init': 1, 'max_iter': 50, 'covariance_type':'diag'})\n",
    "        else:\n",
    "            self.cluster = MiniBatchKMeans(n_clusters=self.config['n_words'], n_init='auto', compute_labels=False, random_state=123)\n",
    "            self.cluster.fit(train_features.reshape(-1,dim_des))\n",
    "        \n",
    "        # Calculate the visual words for each image and level\n",
    "        dim_size = self.config['n_words'] if not self.config[\"fisher\"] else 2*self.config['n_words']*dim_des + self.config['n_words'] # n_words or 2KD+K for fisher\n",
    "        visual_words_train = np.zeros((n, self.n_patches_per_image, dim_size), dtype=np.float64) # we will need shape (n_images, dim_size)\n",
    "\n",
    "        for i in range(n):\n",
    "            for patch in range(self.n_patches_per_image):\n",
    "                words = self.cluster.predict(train_features[i,patch]) if not self.config[\"fisher\"] else fisher_vector(train_features[i,patch], self.cluster)\n",
    "                hist = np.bincount(words, minlength=self.config['n_words'])\n",
    "                hist /= sum(hist)\n",
    "                print(hist)\n",
    "                visual_words_train[i,patch,:] = hist if not self.config[\"fisher\"] else words\n",
    "\n",
    "        visual_words_train = visual_words_train.reshape(n, -1) #get shape (n_images, dim_size)\n",
    "\n",
    "        # Standarization\n",
    "        if self.config['scaler']:\n",
    "            visual_words_train = self.scaler.fit_transform(visual_words_train)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        if self.dim_red is not None:\n",
    "            visual_words_train = self.dim_red.fit_transform(visual_words_train)\n",
    "        \n",
    "        # Compute distance/kenrel matrix\n",
    "        if self.config['classifier'] == 'knn' and self.config['metric'] == 'precomputed':\n",
    "            self.visual_words_train_old = visual_words_train.copy()\n",
    "            visual_words_train = histogram_intersection_distance(visual_words_train, visual_words_train)\n",
    "        elif self.config['classifier'] == 'svm' and self.config['kernel'] == 'precomputed':\n",
    "            self.visual_words_train_old = visual_words_train.copy()\n",
    "            visual_words_train = histogram_intersection_kernel(visual_words_train, visual_words_train)\n",
    "            \n",
    "        # Fit the classifier\n",
    "        self.classifier.fit(visual_words_train, y_train_labels)\n",
    "\n",
    "config = {\n",
    "    'patch_size': 64,\n",
    "    'overlap': 16, # overlap proportion of patches -> 4 means 1/4 of the patch is overlapped\n",
    "    'step_size': 32, # distance between dense sift keypoints\n",
    "    'kp_scale': 8, # size of dense sift keypoints\n",
    "    'out_size': 2048, # size of the output of the mlp\n",
    "    'num_layers': 3, # number of layers of the mlp\n",
    "    'features': 'mlp',\n",
    "    'classifier': 'logistic',\n",
    "    'scaler': True,\n",
    "    'n_components': 0,\n",
    "    'n_neigh': 1,\n",
    "    'metric': 'euclidean',\n",
    "    'kernel': 'linear',\n",
    "    'degree_pol': 3,\n",
    "    'C': 1,\n",
    "}\n",
    "bovw = MLP_BoVW(config, data_path='./MIT_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1881, 16, 4, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bovw.train_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
